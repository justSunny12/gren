# /services/model_service_mlx.py
import mlx.core as mx
import mlx.nn as nn
from mlx_lm import load, generate
import time
import platform
from typing import Dict, Any, List
from threading import Lock
from datetime import datetime
import json

class ModelServiceMLX:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ MLX"""
    
    def __init__(self):
        self.config = None
        self.model = None
        self.tokenizer = None
        self.generate_lock = Lock()
        self._initialized = False
        self._warming_up = False
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.generation_stats = {
            'total_requests': 0,
            'avg_generation_time': 0,
            'total_tokens_generated': 0,
            'last_cleanup': datetime.now()
        }
        
    def initialize(self, force_reload: bool = False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ MLX
        """
        if self._initialized and not force_reload:
            return self.model, self.tokenizer, self.generate_lock
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        from container import container
        self.config = container.get_config()
        model_config = self.config.get("model", {})
        
        start_time = time.time()
        
        try:
            print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_config.get('name', 'Qwen/Qwen3-4B')} —á–µ—Ä–µ–∑ MLX...")
            print(f"   device: MLX (Apple Silicon)")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ mlx-lm
            model_name = model_config.get("name", "Qwen/Qwen3-4B")
            
            print("   ‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏... (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è)")
            
            self.model, self.tokenizer = load(model_name)
            
            self._initialized = True
            
            load_time = time.time() - start_time
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥")
            
            # –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏
            print("üî• –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏...")
            try:
                self._warming_up = True
                warmup_prompt = [{"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç"}]
                warmup_response = self.generate_response(
                    warmup_prompt, 
                    max_tokens=10,
                    temperature=0.1
                )
                self._warming_up = False
                print("‚úÖ –ú–æ–¥–µ–ª—å –ø—Ä–æ–≥—Ä–µ—Ç–∞ —É—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                print(f"‚ÑπÔ∏è –ü—Ä–æ–≥—Ä–µ–≤ –Ω–µ —É–¥–∞–ª—Å—è: {e}, –Ω–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            return self.model, self.tokenizer, self.generate_lock
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ MLX: {e}")
            import traceback
            traceback.print_exc()
            return None, None, self.generate_lock
    
    def get_generation_params(self, max_tokens: int = None, 
                             temperature: float = None) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è MLX
        """
        if not self._initialized:
            self.initialize()
        
        if max_tokens is None:
            max_tokens = self.config.get("generation", {}).get("default_max_tokens", 512)
        if temperature is None:
            temperature = self.config.get("generation", {}).get("default_temperature", 0.7)
        
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è MLX
        params = {
            "max_tokens": max_tokens,
            "temp": max(temperature, 0.01),
            "top_p": self.config.get("generation", {}).get("default_top_p", 0.9),
        }
        
        return params
    
    def generate_response(self, messages: list, max_tokens: int = 512, 
                        temperature: float = 0.7, enable_thinking: bool = False) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ MLX
        """
        if not self._initialized:
            self.initialize()
        
        self.generation_stats['total_requests'] += 1
        
        # –í—ã–∫–ª—é—á–∞–µ–º Thinking –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞
        if self._warming_up:
            enable_thinking = False
        
        try:
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            if hasattr(self.tokenizer, 'apply_chat_template'):
                try:
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å enable_thinking –µ—Å–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç
                    prompt = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True,
                        enable_thinking=enable_thinking
                    )
                except TypeError:
                    # –ï—Å–ª–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç enable_thinking
                    prompt = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
            else:
                # Fallback –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
                prompt = ""
                for msg in messages:
                    if msg["role"] == "user":
                        prompt += f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {msg['content']}\n"
                    elif msg["role"] == "assistant":
                        prompt += f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {msg['content']}\n"
                prompt += "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: "
        
        except Exception as e:
            if not self._warming_up:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞: {e}")
            prompt = messages[-1]["content"] if messages else ""
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        params = self.get_generation_params(max_tokens, temperature)
        
        start_time = time.time()
        
        with self.generate_lock:
            try:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ MLX
                response = generate(
                    model=self.model,
                    tokenizer=self.tokenizer,
                    prompt=prompt,
                    max_tokens=params["max_tokens"],
                    temp=params["temp"],
                    top_p=params["top_p"],
                )
                
                response_tokens = len(self.tokenizer.encode(response))
                self.generation_stats['total_tokens_generated'] += response_tokens
                    
            except Exception as e:
                if not self._warming_up:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                response = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."
        
        generation_time = time.time() - start_time
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        if self.generation_stats['total_requests'] > 1:
            old_avg = self.generation_stats['avg_generation_time']
            new_count = self.generation_stats['total_requests']
            self.generation_stats['avg_generation_time'] = (
                old_avg * (new_count - 1) + generation_time
            ) / new_count
        
        return response
    
    def get_stats(self) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        if self.generation_stats['total_requests'] > 0:
            tokens_per_request = (
                self.generation_stats['total_tokens_generated'] / 
                self.generation_stats['total_requests']
                if self.generation_stats['total_requests'] > 0 else 0
            )
        else:
            tokens_per_request = 0
        
        return {
            'device': 'mlx',
            'total_requests': self.generation_stats['total_requests'],
            'avg_generation_time_ms': self.generation_stats['avg_generation_time'] * 1000,
            'total_tokens_generated': self.generation_stats['total_tokens_generated'],
            'tokens_per_request': tokens_per_request,
            'model_initialized': self._initialized,
        }
    
    def is_initialized(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å"""
        return self._initialized

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ container.py
model_service_mlx = ModelServiceMLX()