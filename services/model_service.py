# /services/model_service.py
import torch
import gc
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from threading import Lock
from typing import Tuple, Any, Dict, Optional

class ModelService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å ML –º–æ–¥–µ–ª—å—é"""
    
    def __init__(self):
        self.config = None
        self.model = None
        self.tokenizer = None
        self.generator = None
        self.generate_lock = Lock()
        self._initialized = False
        self.device = self._get_device()
    
    def _get_device(self):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª—É—á—à–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"""
        if torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print("Device set to use mps")
            return "mps"
        else:
            return "cpu"
    
    def _load_config(self):
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞"""
        if self.config is None:
            from container import container
            self.config = container.get_config()
    
    def initialize(self) -> Tuple[Any, Any, Lock]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä"""
        if self._initialized:
            return self.model, self.tokenizer, self.generate_lock
        
        self._load_config()
        model_config = self.config.model
        
        print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å Pipeline (device: {self.device})...", end="", flush=True)
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º dtype
            dtype_map = {
                "float16": torch.float16,
                "bfloat16": torch.bfloat16,
                "auto": torch.float16 if self.device in ["cuda", "mps"] else torch.float32
            }
            dtype = dtype_map.get(model_config.dtype.value, torch.float16)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_config.name,
                padding_side="left",
                trust_remote_code=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º dtype –≤–º–µ—Å—Ç–æ torch_dtype
            self.generator = pipeline(
                "text-generation",
                model=model_config.name,
                tokenizer=self.tokenizer,
                dtype=dtype,
                device=self.device,
                model_kwargs={
                    "attn_implementation": model_config.attn_implementation,
                    "low_cpu_mem_usage": model_config.low_cpu_mem_usage,
                    "trust_remote_code": True,
                }
            )
            
            self.model = self.generator.model
            self.model.eval()
            
            self._initialized = True
            print(f" ‚úÖ –ú–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ {self.device}")
            
            return self.model, self.tokenizer, self.generate_lock
            
        except Exception as e:
            print(f" ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return None, None, self.generate_lock
    
    def generate_response(self, messages: list, max_tokens: int = 512, 
                        temperature: float = 0.7, enable_thinking: bool = False) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –Ω–æ–≤—ã–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º
        """
        if not self._initialized:
            self.initialize()
        
        # –¢–∏—Ö–∏–π —Ä–µ–∂–∏–º –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞
        if hasattr(self, '_warming_up') and self._warming_up:
            enable_thinking = False  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤—ã–∫–ª—é—á–∞–µ–º thinking –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º enable_thinking –µ—Å–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=enable_thinking
            )
        except TypeError:
            # Fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        params = self.get_generation_params(max_tokens=max_tokens, temperature=temperature)
        
        # –î–ª—è MPS –ø—Ä–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if self.device == "mps" and hasattr(self, '_warming_up') and self._warming_up:
            params = {"max_new_tokens": max_tokens}
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ –ø–∞–π–ø–ª–∞–π–Ω
        response = self.generate_with_pipeline(text, **params)
        
        return response
    
    def generate_with_pipeline(self, prompt: str, **generation_params) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Pipeline"""
        if not self._initialized:
            self.initialize()
        
        with self.generate_lock:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                results = self.generator(
                    prompt,
                    **generation_params,
                    return_full_text=False
                )
                
                if isinstance(results, list) and len(results) > 0:
                    return results[0]['generated_text']
                return ""
                
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ generate_with_pipeline: {e}")
                return ""
    
    def get_generation_params(self, **overrides) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è Pipeline"""
        if not self._initialized:
            self.initialize()
        
        self._load_config()
        gen_config = self.config.generation
        
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –≤—Å–µ–º–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏)
        params = {
            "max_new_tokens": overrides.get("max_tokens", gen_config.default_max_tokens),
            "temperature": overrides.get("temperature", gen_config.default_temperature),
            "do_sample": True,
            "num_return_sequences": 1,
        }
        
        # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if self.device == "cuda":
            params.update({
                "top_p": gen_config.default_top_p,
                "repetition_penalty": gen_config.default_repetition_penalty,
            })
        elif self.device == "cpu":
            params.update({
                "top_p": gen_config.default_top_p,
            })
        # –î–ª—è MPS –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω—ã –µ—Å–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
        if self.tokenizer:
            params["pad_token_id"] = self.tokenizer.pad_token_id
            params["eos_token_id"] = self.tokenizer.eos_token_id
        
        return params
    
    def chat_with_pipeline(self, messages: list, **generation_params) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –¥–ª—è —á–∞—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Pipeline"""
        if not self._initialized:
            self.initialize()
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –ø–æ–º–æ—â—å—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å –ø–∞–π–ø–ª–∞–π–Ω–æ–º
        response = self.generate_with_pipeline(text, **generation_params)
        return response
    
    def is_initialized(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å"""
        return self._initialized
    
    def get_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–∞–∑–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        return {
            'device': self.device,
            'model_initialized': self._initialized,
            'service_type': 'ModelService',
        }
    
    def cleanup(self):
        """
        –û—á–∏—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        –ú–æ–¥–µ–ª—å –æ—Å—Ç–∞–µ—Ç—Å—è –≤ –ø–∞–º—è—Ç–∏
        """
        print("üßπ –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ModelService...")
        
        # –û—á–∏—â–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –∫—ç—à (–Ω–µ –º–æ–¥–µ–ª—å!)
        if self.device == "cuda":
            torch.cuda.empty_cache()
        elif self.device == "mps":
            torch.mps.empty_cache()
        
        gc.collect()
        
        print("‚úÖ –í—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã (–º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç–∏)")
    
    def force_cleanup(self):
        """
        –ü–û–õ–ù–ê–Ø –æ—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
        –¢–æ–ª—å–∫–æ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        """
        print("üßπ –ü–û–õ–ù–ê–Ø –æ—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ ModelService...")
        
        if self.generator:
            try:
                # –î–ª—è MPS –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º
                if self.device == "mps" and hasattr(self.generator.model, 'to'):
                    self.generator.model = self.generator.model.to('cpu')
                
                del self.generator
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞: {e}")
            finally:
                self.generator = None
        
        if self.model:
            try:
                del self.model
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
            finally:
                self.model = None
        
        if self.tokenizer:
            try:
                del self.tokenizer
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}")
            finally:
                self.tokenizer = None
        
        # –°–∏—Å—Ç–µ–º–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        if self.device == "cuda":
            torch.cuda.empty_cache()
        elif self.device == "mps":
            torch.mps.empty_cache()
        
        gc.collect()
        
        self._initialized = False
        print("‚úÖ –í—Å–µ —Ä–µ—Å—É—Ä—Å—ã ModelService –≤—ã–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –ø–∞–º—è—Ç–∏")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –µ—Å–ª–∏ –≥–¥–µ-—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
model_service = ModelService()