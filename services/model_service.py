# /services/model_service.py
import torch
import gc
import time
import platform
from typing import Dict, Any, List, Optional
from threading import Lock
from datetime import datetime
from transformers import pipeline, AutoTokenizer
import psutil

class ModelService:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏
    –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –∏ –æ—Å—Ç–∞–µ—Ç—Å—è –≤ –ø–∞–º—è—Ç–∏
    """
    
    def __init__(self):
        self.config = None
        self.generator = None
        self.tokenizer = None
        self.generate_lock = Lock()
        self._initialized = False
        self._warming_up = False
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.device = self._get_device()
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.generation_stats = {
            'total_requests': 0,
            'batch_requests': 0,
            'avg_generation_time': 0,
            'total_tokens_generated': 0,
            'last_cleanup': datetime.now()
        }
        
        # –ö—ç—à –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ - –ù–ò–ö–û–ì–î–ê –Ω–µ –æ—á–∏—â–∞–µ–º –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏!
        self.param_cache = {}
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –±—É—Ñ–µ—Ä—ã (–º–æ–∂–Ω–æ –æ—á–∏—â–∞—Ç—å)
        self.temp_buffers = []
        self.temp_tensors = []
        
        # Batch –æ—á–µ—Ä–µ–¥—å
        self.batch_size = 1
    
    def _get_device(self):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª—É—á—à–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∑–∞–ø—É—Å–∫–∞"""
        if torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print("Device set to use mps")
            return "mps"
        else:
            return "cpu"
    
    def _get_dtype(self):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π dtype –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        if self.device == "cuda":
            return torch.float16
        elif self.device == "mps":
            return torch.float16
        else:
            return torch.float32
    
    def initialize(self, force_reload: bool = False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        """
        if self._initialized and not force_reload:
            return self.generator.model if self.generator else None, self.tokenizer, self.generate_lock
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        from container import container
        self.config = container.get_config()
        model_config = self.config.model
        
        start_time = time.time()
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π dtype –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
            dtype = self._get_dtype()
            
            print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_config.name}...")
            print(f"   device: {self.device}")
            print(f"   dtype: {dtype}")
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤
            if self.device == "cuda":
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.benchmark = True
            elif self.device == "mps":
                torch.mps.empty_cache()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_config.name,
                trust_remote_code=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.padding_side = "left"
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è pipeline
            model_kwargs = {
                "attn_implementation": model_config.attn_implementation,
                "low_cpu_mem_usage": model_config.low_cpu_mem_usage,
            }
            
            if self.device == "cuda":
                model_kwargs["device_map"] = "auto"
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑
            self.generator = pipeline(
                "text-generation",
                model=model_config.name,
                tokenizer=self.tokenizer,
                device=self.device if self.device != "mps" else -1,
                batch_size=self.batch_size,
                model_kwargs=model_kwargs
            )
            
            self._initialized = True
            
            load_time = time.time() - start_time
                        
            return self.generator.model, self.tokenizer, self.generate_lock
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return None, None, self.generate_lock
    
    def get_generation_params(self, max_tokens: Optional[int] = None, 
                             temperature: Optional[float] = None) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        if not self._initialized:
            self.initialize()
        
        if max_tokens is None:
            max_tokens = self.config.generation.default_max_tokens
        if temperature is None:
            temperature = self.config.generation.default_temperature
        
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        params = {
            "max_new_tokens": max_tokens,
            "temperature": max(temperature, 0.01),
            "do_sample": temperature > 0.1,
        }
        
        # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if self.device == "cuda":
            params.update({
                "top_p": self.config.generation.default_top_p,
                "repetition_penalty": self.config.generation.default_repetition_penalty,
            })
        elif self.device == "cpu":
            params.update({
                "top_p": self.config.generation.default_top_p,
            })
        # MPS –æ—Å—Ç–∞–≤–ª—è–µ–º —Å –±–∞–∑–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        
        if self.tokenizer:
            params["pad_token_id"] = self.tokenizer.pad_token_id
            params["eos_token_id"] = self.tokenizer.eos_token_id
        
        return params
    
    def generate_response(self, messages: list, max_tokens: int = 512, 
                        temperature: float = 0.7, enable_thinking: bool = False) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º enable_thinking –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ Qwen
        """
        if not self._initialized:
            self.initialize()
        
        self.generation_stats['total_requests'] += 1
        
        # –í—ã–∫–ª—é—á–∞–µ–º Thinking –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞
        if hasattr(self, '_warming_up') and self._warming_up:
            # –¢–∏—Ö–∏–π —Ä–µ–∂–∏–º –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞
            enable_thinking = False  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤—ã–∫–ª—é—á–∞–µ–º thinking –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä enable_thinking —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ Qwen
            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=enable_thinking  # ‚Üê –®—Ç–∞—Ç–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –º–æ–¥–µ–ª–∏
            )
        except TypeError as e:
            # –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç enable_thinking (—Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è)
            if not (hasattr(self, '_warming_up') and self._warming_up):
                print(f"‚ö†Ô∏è –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç enable_thinking: {e}")
            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        params = self.get_generation_params(max_tokens, temperature)
        
        start_time = time.time()
        
        with self.generate_lock:
            try:
                # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                gen_params = {
                    "max_new_tokens": params["max_new_tokens"],
                    "temperature": params.get("temperature", 0.7),
                    "do_sample": params.get("do_sample", True),
                    "return_full_text": False,
                }
                
                # –î–ª—è MPS —É–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≤—ã–∑—ã–≤–∞—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
                if self.device == "cuda":
                    gen_params.update({
                        "top_p": params.get("top_p", 0.9),
                        "repetition_penalty": params.get("repetition_penalty", 1.1),
                    })
                elif self.device == "mps":
                    # MPS –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                    # –£–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—ã–∑—ã–≤–∞—é—â–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –ø—Ä–∏ –ø—Ä–æ–≥—Ä–µ–≤–µ
                    if hasattr(self, '_warming_up') and self._warming_up:
                        gen_params = {
                            "max_new_tokens": params["max_new_tokens"],
                            "return_full_text": False,
                        }
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º
                results = self.generator(prompt, **gen_params)
                
                if results and len(results) > 0:
                    response = results[0]['generated_text'].strip()
                    response_tokens = len(self.tokenizer.encode(response))
                    self.generation_stats['total_tokens_generated'] += response_tokens
                else:
                    response = ""
                    
            except Exception as e:
                if not (hasattr(self, '_warming_up') and self._warming_up):
                    print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                response = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."
        
        generation_time = time.time() - start_time
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        if self.generation_stats['total_requests'] > 1:
            old_avg = self.generation_stats['avg_generation_time']
            new_count = self.generation_stats['total_requests']
            self.generation_stats['avg_generation_time'] = (
                old_avg * (new_count - 1) + generation_time
            ) / new_count
        
        return response
    
    def _print_memory_info(self):
        """–í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏"""
        try:
            # –ù–µ –≤—ã–≤–æ–¥–∏–º –µ—Å–ª–∏ –≤ —Ä–µ–∂–∏–º–µ –ø—Ä–æ–≥—Ä–µ–≤–∞
            if hasattr(self, '_warming_up') and self._warming_up:
                return
                
            if self.device == "cuda":
                gpu_memory = torch.cuda.memory_allocated() / 1024**3
                gpu_memory_max = torch.cuda.max_memory_allocated() / 1024**3
                print(f"üíæ GPU –ø–∞–º—è—Ç—å: {gpu_memory:.2f} GB (–ø–∏–∫: {gpu_memory_max:.2f} GB)")
            elif self.device == "mps":
                print(f"üíæ MPS —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: –¥–æ—Å—Ç—É–ø–Ω–æ")
            
            process = psutil.Process()
            memory_info = process.memory_info()
            ram_usage = memory_info.rss / 1024**3
            print(f"üíæ RAM –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: {ram_usage:.2f} GB")
            
        except Exception as e:
            # –ù–µ –≤—ã–≤–æ–¥–∏–º –æ—à–∏–±–∫—É –µ—Å–ª–∏ –≤ —Ä–µ–∂–∏–º–µ –ø—Ä–æ–≥—Ä–µ–≤–∞
            if not (hasattr(self, '_warming_up') and self._warming_up):
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        if self.generation_stats['total_requests'] > 0:
            tokens_per_request = (
                self.generation_stats['total_tokens_generated'] / 
                self.generation_stats['total_requests']
                if self.generation_stats['total_requests'] > 0 else 0
            )
        else:
            tokens_per_request = 0
        
        return {
            'device': self.device,
            'total_requests': self.generation_stats['total_requests'],
            'batch_requests': self.generation_stats['batch_requests'],
            'avg_generation_time_ms': self.generation_stats['avg_generation_time'] * 1000,
            'total_tokens_generated': self.generation_stats['total_tokens_generated'],
            'tokens_per_request': tokens_per_request,
            'param_cache_size': len(self.param_cache),
            'model_initialized': self._initialized,
        }
    
    def cleanup(self):
        """
        –û—á–∏—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        –ú–æ–¥–µ–ª—å –∏ –∫—ç—à –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ—Å—Ç–∞—é—Ç—Å—è –≤ –ø–∞–º—è—Ç–∏
        """
        print("üßπ –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        # –û—á–∏—â–∞–µ–º —Ç–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±—É—Ñ–µ—Ä—ã
        self.temp_buffers.clear()
        
        # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã
        for tensor in self.temp_tensors:
            try:
                if hasattr(tensor, 'detach'):
                    tensor.detach()
                if hasattr(tensor, 'cpu'):
                    tensor.cpu()
            except:
                pass
        self.temp_tensors.clear()
        
        # –û—á–∏—â–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –∫—ç—à (–Ω–µ –º–æ–¥–µ–ª—å!)
        if self.device == "cuda":
            torch.cuda.empty_cache()
        elif self.device == "mps":
            torch.mps.empty_cache()
        
        gc.collect()
        
        self.generation_stats['last_cleanup'] = datetime.now()
        print("‚úÖ –í—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã (–º–æ–¥–µ–ª—å –∏ –∫—ç—à –≤ –ø–∞–º—è—Ç–∏)")
    
    def force_cleanup(self):
        """
        –ü–û–õ–ù–ê–Ø –æ—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
        –¢–û–õ–¨–ö–û –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        """
        print("üßπ –ü–û–õ–ù–ê–Ø –æ—á–∏—Å—Ç–∫–∞ –í–°–ï–• —Ä–µ—Å—É—Ä—Å–æ–≤ –º–æ–¥–µ–ª–∏...")
        
        if self.generator:
            try:
                # –î–ª—è MPS –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º
                if self.device == "mps" and hasattr(self.generator.model, 'to'):
                    self.generator.model = self.generator.model.to('cpu')
                
                del self.generator
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞: {e}")
            finally:
                self.generator = None
        
        if self.tokenizer:
            try:
                del self.tokenizer
            except:
                pass
            self.tokenizer = None
        
        # –û—á–∏—â–∞–µ–º –í–°–ï –∫—ç—à–∏ (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏!)
        self.param_cache.clear()
        self.temp_buffers.clear()
        self.temp_tensors.clear()
        
        # –°–∏—Å—Ç–µ–º–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        if self.device == "cuda":
            torch.cuda.empty_cache()
        elif self.device == "mps":
            torch.mps.empty_cache()
        
        gc.collect()
        
        self._initialized = False
        print("‚úÖ –í—Å–µ —Ä–µ—Å—É—Ä—Å—ã –º–æ–¥–µ–ª–∏ –≤—ã–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –ø–∞–º—è—Ç–∏")
    
    def is_initialized(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å"""
        return self._initialized