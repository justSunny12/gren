# /services/model_service.py
import time
import os
import re
from typing import Dict, Any, List, Optional
from threading import Lock
from datetime import datetime
import mlx.core as mx
from mlx_lm import load, generate
from mlx_lm.sample_utils import make_sampler, make_logits_processors

class ModelService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏ –Ω–∞ MLX —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π enable_thinking"""

    def __init__(self):
        # –õ–µ–Ω–∏–≤–æ –∑–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –∏ —Å–µ—Ä–≤–∏—Å—ã
        self._config = None
        self.model = None
        self.tokenizer = None
        self.generate_lock = Lock()
        self._initialized = False
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.generation_stats = {
            'total_requests': 0,
            'avg_generation_time': 0,
            'total_tokens_generated': 0,
            'last_cleanup': datetime.now()
        }

    @property
    def config(self) -> Dict[str, Any]:
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if self._config is None:
            from container import container
            self._config = container.get_config()
        return self._config

    def _setup_memory_limit(self):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏ –¥–ª—è MLX –Ω–∞ Apple Silicon"""
        model_config = self.config.get("model", {})
        memory_limit = model_config.get("unified_memory_limit")
        
        if memory_limit and hasattr(mx.metal, 'set_cache_limit'):
            try:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º MB –≤ –±–∞–π—Ç—ã
                total_memory = mx.device_info().get('memory_size', 0)
                limit_bytes = int(total_memory * 0.8)
                mx.set_cache_limit(limit_bytes)
                print(f"üíæ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏ MLX: {limit_bytes/1024**3:.2f} GB")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏: {e}")

    def initialize(self, force_reload: bool = False):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å"""
        if self._initialized and not force_reload:
            return self.model, self.tokenizer, self.generate_lock

        model_config = self.config.get("model", {})
        start_time = time.time()

        try:
            # 1. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏
            self._setup_memory_limit()
            
            # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            local_path = model_config.get("local_path")
            model_name = model_config.get("name", "Qwen/Qwen3-30B-A3B-MLX-4bit")

            # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å
            load_path = None
            if local_path and os.path.exists(local_path):
                load_path = local_path
                print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏: {local_path}")
            elif local_path:
                # –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å —É–∫–∞–∑–∞–Ω, –Ω–æ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                print(f"‚ö†Ô∏è –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {local_path}")
                print(f"üì° –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ Hugging Face: {model_name}")
                load_path = model_name
            else:
                # –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –Ω–µ —É–∫–∞–∑–∞–Ω - –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ HF
                print(f"üì° –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ Hugging Face: {model_name}")
                load_path = model_name
            
            # 4. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ load)
            self.model, self.tokenizer = load(
                model_name
            )

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.padding_side = "left"

            self._initialized = True
            load_time = time.time() - start_time
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥")
            
            # 5. –ï—Å–ª–∏ —Å–∫–∞—á–∞–ª–∏ –∏–∑ HF –∏ —É–∫–∞–∑–∞–Ω –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ
            if load_path == model_name and local_path and not os.path.exists(local_path):
                print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å: {local_path}")
                self._save_model_locally(local_path)

            return self.model, self.tokenizer, self.generate_lock

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            import traceback
            traceback.print_exc()
            return None, None, self.generate_lock

    def _save_model_locally(self, local_path: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        try:
            os.makedirs(local_path, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            if hasattr(self.model, 'save_pretrained'):
                self.model.save_pretrained(local_path)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            if hasattr(self.tokenizer, 'save_pretrained'):
                self.tokenizer.save_pretrained(local_path)
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {local_path}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ: {e}")

    def _get_generation_parameters(
        self, 
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        enable_thinking: Optional[bool] = None
    ) -> Dict[str, Any]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å —É—á—ë—Ç–æ–º –∫–æ–Ω—Ñ–∏–≥–∞ –∏ —Ä–µ–∂–∏–º–∞"""
        
        gen_config = self.config.get("generation", {})
        thinking_config = gen_config.get("thinking_params", {})
        
        # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π
        use_thinking = enable_thinking if enable_thinking is not None \
            else gen_config.get("default_enable_thinking", False)
        
        # 2. –í—ã–±–∏—Ä–∞–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –∏ top_p –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
        if use_thinking:
            final_temp = temperature if temperature is not None \
                else thinking_config.get("temperature", 0.6)
            final_top_p = thinking_config.get("top_p", 0.95)
        else:
            final_temp = temperature if temperature is not None \
                else gen_config.get("default_temperature", 0.7)
            final_top_p = gen_config.get("default_top_p", 0.8)
        
        # 3. –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        final_max_tokens = max_tokens if max_tokens is not None \
            else gen_config.get("default_max_tokens", 512)
        repetition_penalty = gen_config.get("repetition_penalty", 1.1)
        top_k = gen_config.get("top_k", 40)
        
        return {
            "max_tokens": final_max_tokens,
            "temperature": final_temp,
            "top_p": final_top_p,
            "repetition_penalty": repetition_penalty,
            "top_k": top_k,
            "enable_thinking": use_thinking
        }

    def generate_response(
        self, 
        messages: List[Dict[str, str]], 
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        enable_thinking: Optional[bool] = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å —É—á—ë—Ç–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞"""
        
        if not self._initialized:
            self.initialize()
        
        self.generation_stats['total_requests'] += 1
        params = self._get_generation_parameters(max_tokens, temperature, enable_thinking)
        
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å —É—á—ë—Ç–æ–º enable_thinking
            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=params["enable_thinking"]
            )
            
        except Exception as e:
            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç—É—é –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—é, –µ—Å–ª–∏ apply_chat_template –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç enable_thinking
            prompt = "\n".join([f"{m['role']}: {m['content']}" for m in messages])
            prompt += "\nassistant: "
            if params["enable_thinking"]:
                prompt += "<think>"
        
        # –°–æ–∑–¥–∞—ë–º —Å—ç–º–ø–ª–µ—Ä –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        sampler = make_sampler(
            temp=params["temperature"],
            top_p=params["top_p"],
            top_k=params["top_k"]
        )
        
        logits_processors = make_logits_processors(
            repetition_penalty=params["repetition_penalty"]
        )
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        start_time = time.time()
        
        with self.generate_lock:
            try:
                response = generate(
                    model=self.model,
                    tokenizer=self.tokenizer,
                    prompt=prompt,
                    sampler=sampler,
                    logits_processors=logits_processors,
                    max_tokens=params["max_tokens"],
                    verbose=False
                )
                
                # –£–¥–∞–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ—Å—Ç–∞—Ç–∫–∏ —Ç–µ–≥–æ–≤ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π
                response_text = self._clean_thinking_tags(response.strip())
                response_tokens = len(self.tokenizer.encode(response_text))
                self.generation_stats['total_tokens_generated'] += response_tokens
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                response_text = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."
        
        generation_time = time.time() - start_time
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        if self.generation_stats['total_requests'] > 1:
            old_avg = self.generation_stats['avg_generation_time']
            new_count = self.generation_stats['total_requests']
            self.generation_stats['avg_generation_time'] = (
                old_avg * (new_count - 1) + generation_time
            ) / new_count
        
        return response_text

    def _clean_thinking_tags(self, text: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º HTML span"""
        import re
        
        think_pattern = r'<think>(.*?)</think>'
        
        def replace_with_span(match):
            think_text = match.group(1).strip()
            if not think_text:
                return ""
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –∏ –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ span
            lines = think_text.split('\n')
            span_lines = []
            for line in lines:
                line = line.strip()
                if line:
                    span_lines.append(f"<span class='thinking-text'>{line}</span>")
                else:
                    span_lines.append('')
            
            return '\n'.join(span_lines)
        
        text = re.sub(think_pattern, replace_with_span, text, flags=re.DOTALL)
        
        # –£–¥–∞–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Ç–µ–≥–∏
        text = text.replace('<think>', '').replace('</think>', '')
        
        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text.strip()

    def get_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        if self.generation_stats['total_requests'] > 0:
            tokens_per_request = (
                self.generation_stats['total_tokens_generated'] / 
                self.generation_stats['total_requests']
                if self.generation_stats['total_requests'] > 0 else 0
            )
        else:
            tokens_per_request = 0
        
        return {
            'backend': 'mlx',
            'total_requests': self.generation_stats['total_requests'],
            'avg_generation_time_ms': round(self.generation_stats['avg_generation_time'] * 1000, 2),
            'total_tokens_generated': self.generation_stats['total_tokens_generated'],
            'tokens_per_request': round(tokens_per_request, 2),
            'model_initialized': self._initialized,
        }

    def is_initialized(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å"""
        return self._initialized