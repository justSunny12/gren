# /services/model_service.py (—É–º–µ–Ω—å—à–∏–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ)
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from threading import Lock
from typing import Tuple, Any

class ModelService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å ML –º–æ–¥–µ–ª—å—é"""
    
    def __init__(self):
        self.config = None
        self.model = None
        self.tokenizer = None
        self.generate_lock = Lock()
        self._initialized = False
    
    def _load_config(self):
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞"""
        if self.config is None:
            from container import container
            self.config = container.get_config()
    
    def initialize(self) -> Tuple[Any, Any, Lock]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ)"""
        if self._initialized:
            return self.model, self.tokenizer, self.generate_lock
        
        self._load_config()
        model_config = self.config.model
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...", end="", flush=True)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º dtype –¥–ª—è —Ç–µ–Ω–∑–æ—Ä–æ–≤
        dtype_map = {
            "float16": torch.float16,
            "bfloat16": torch.bfloat16,
            "auto": None
        }
        dtype = dtype_map.get(model_config.dtype.value, torch.bfloat16)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–±–µ–∑ —Å–æ–æ–±—â–µ–Ω–∏–π)
        self.tokenizer = AutoTokenizer.from_pretrained(model_config.name)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å (–±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_config.name,
            device_map="auto",
            dtype=dtype,
            attn_implementation=model_config.attn_implementation,
            low_cpu_mem_usage=model_config.low_cpu_mem_usage
        )
        
        # –í–∫–ª—é—á–∞–µ–º —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏
        self.model.eval()
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è Apple Silicon
        torch.set_num_threads(torch.get_num_threads())
        torch.set_num_interop_threads(1)
        
        self._initialized = True
        print(" ‚úÖ –ú–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")  # –ó–∞–≤–µ—Ä—à–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≥–∞–ª–æ—á–∫–æ–π
        
        return self.model, self.tokenizer, self.generate_lock
    
    def get_generation_params(self, **overrides):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"""
        if not self._initialized:
            self.initialize()
        
        self._load_config()
        gen_config = self.config.generation
        
        params = {
            "max_new_tokens": overrides.get("max_tokens", gen_config.default_max_tokens),
            "temperature": overrides.get("temperature", gen_config.default_temperature),
            "top_p": gen_config.default_top_p,
            "repetition_penalty": gen_config.default_repetition_penalty,
            "do_sample": True,
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º pad_token_id, –µ—Å–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
        if self.tokenizer:
            params["pad_token_id"] = self.tokenizer.pad_token_id or self.tokenizer.eos_token_id
        
        return params
    
    def is_initialized(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å"""
        return self._initialized
    
    def cleanup(self):
        """–û—á–∏—â–∞–µ—Ç —Ä–µ—Å—É—Ä—Å—ã –º–æ–¥–µ–ª–∏"""
        if self.model:
            del self.model
            self.model = None
        
        if self.tokenizer:
            del self.tokenizer
            self.tokenizer = None
        
        self._initialized = False
        print("üßπ –†–µ—Å—É—Ä—Å—ã –º–æ–¥–µ–ª–∏ –æ—á–∏—â–µ–Ω—ã")