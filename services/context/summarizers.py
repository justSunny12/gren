# services/context/summarizers.py
"""
–°–µ—Ä–≤–∏—Å—ã —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ MLX
"""
import asyncio
import threading
import time
from typing import Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import re
import os

import mlx.core as mx
import mlx.nn as nn
from mlx_lm import load, generate
from mlx_lm.sample_utils import make_sampler, make_logits_processors

from container import container


@dataclass
class SummaryResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    summary: str
    original_length: int
    summary_length: int
    compression_ratio: float
    processing_time: float
    success: bool
    error: Optional[str] = None


class BaseSummarizer:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤ —Å –∑–∞–≥—Ä—É–∑–∫–æ–π –¢–û–õ–¨–ö–û –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏"""
    
    def __init__(self, model_config: Dict[str, Any], config: Dict[str, Any]):
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        self.model_name = model_config.get("name", "unknown")
        self.local_path = model_config.get("local_path")
        self.config = config
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self._model = None
        self._tokenizer = None
        self._model_lock = threading.RLock()
        self._is_loading = False
        self._load_error = None
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        summarization_params = config.get("models", {}).get("generation_params", {})
        model_type = "l1" if "1.7B" in self.model_name else "l2"
        params = summarization_params.get(model_type, {})
        
        self.max_tokens = params.get("max_tokens", 200)
        self.temperature = params.get("temperature", 0.3)
        self.top_p = params.get("top_p", 0.9)
        self.top_k = params.get("top_k", 40)
        self.repetition_penalty = params.get("repetition_penalty", 1.1)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self._total_requests = 0
        self._successful_requests = 0
        self._total_processing_time = 0.0
        self._last_used: Optional[float] = None
    
    @property
    def is_loaded(self) -> bool:
        return self._model is not None and self._tokenizer is not None
    
    @property
    def is_loading(self) -> bool:
        return self._is_loading
    
    @property
    def stats(self) -> Dict[str, Any]:
        last_used_iso = None
        if self._last_used:
            try:
                last_used_iso = datetime.fromtimestamp(self._last_used).isoformat()
            except (ValueError, OSError):
                last_used_iso = str(self._last_used)
        return {
            'model_name': self.model_name,
            'is_loaded': self.is_loaded,
            'is_loading': self.is_loading,
            'load_error': self._load_error,
            'total_requests': self._total_requests,
            'successful_requests': self._successful_requests,
            'failed_requests': self._total_requests - self._successful_requests,
            'success_rate': self._successful_requests / max(self._total_requests, 1),
            'avg_processing_time': self._total_processing_time / max(self._successful_requests, 1),
            'last_used': last_used_iso,
            'generation_params': {
                'max_tokens': self.max_tokens,
                'temperature': self.temperature,
                'top_p': self.top_p,
                'top_k': self.top_k,
                'repetition_penalty': self.repetition_penalty,
                'enable_thinking': False
            }
        }
    
    async def load_model(self) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –¢–û–õ–¨–ö–û –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏"""
        with self._model_lock:
            if self.is_loaded:
                return True
            if self._is_loading:
                while self._is_loading:
                    await asyncio.sleep(0.1)
                return self.is_loaded
            self._is_loading = True
            self._load_error = None
            try:
                if not self.local_path:
                    self._load_error = f"–õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –Ω–µ —É–∫–∞–∑–∞–Ω –¥–ª—è –º–æ–¥–µ–ª–∏ {self.model_name}"
                    print(f"‚ùå {self._load_error}")
                    return False
                if not os.path.exists(self.local_path):
                    self._load_error = f"–õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {self.local_path}"
                    print(f"‚ùå {self._load_error}")
                    return False
                start_time = time.time()
                self._model, self._tokenizer = load(self.local_path)
                if self._tokenizer.pad_token is None:
                    self._tokenizer.pad_token = self._tokenizer.eos_token
                self._tokenizer.padding_side = "left"
                load_time = time.time() - start_time
                print(f"   ‚úÖ –ú–æ–¥–µ–ª—å {self.model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫")
                return True
            except Exception as e:
                error_msg = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {self.model_name} –∏–∑ {self.local_path}: {str(e)}"
                print(f"‚ùå {error_msg}")
                self._load_error = error_msg
                return False
            finally:
                self._is_loading = False
    
    async def ensure_loaded(self) -> bool:
        """–£–±–µ–∂–¥–∞–µ—Ç—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞"""
        loading_config = self.config.get("models", {}).get("loading", {})
        preload_enabled = loading_config.get("preload", True)
        if preload_enabled and not self.is_loaded and not self.is_loading:
            print(f"‚ö†Ô∏è –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –≤–∫–ª—é—á–µ–Ω–∞ –≤ –∫–æ–Ω—Ñ–∏–≥–µ, –Ω–æ –º–æ–¥–µ–ª—å {self.model_name} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
        if not self.is_loaded and not self.is_loading:
            return await self.load_model()
        elif self.is_loading:
            while self._is_loading:
                await asyncio.sleep(0.1)
            return self.is_loaded
        return True
    
    def _get_system_prompt(self, **kwargs) -> str:
        raise NotImplementedError("–ú–µ—Ç–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –≤ –ø–æ–¥–∫–ª–∞—Å—Å–µ")
    
    def _get_user_prompt(self, text: str, **kwargs) -> str:
        raise NotImplementedError("–ú–µ—Ç–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –≤ –ø–æ–¥–∫–ª–∞—Å—Å–µ")
    
    def _truncate_text(self, text: str, max_chars: int = 4000) -> str:
        return text  # –£–ø—Ä–æ—â–µ–Ω–æ –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏
    
    # ========== –ò–ó–ú–ï–ù–Å–ù–ù–´–ô –ú–ï–¢–û–î ==========
    async def summarize(
        self,
        text: str,
        system_prompt: Optional[str] = None,
        user_prompt: Optional[str] = None,
        **kwargs
    ) -> SummaryResult:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å —Ä–µ–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
        start_time = time.time()
        self._total_requests += 1
        
        try:
            if not await self.ensure_loaded():
                return SummaryResult(
                    summary="",
                    original_length=len(text),
                    summary_length=0,
                    compression_ratio=1.0,
                    processing_time=time.time() - start_time,
                    success=False,
                    error=f"–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {self._load_error}"
                )
            
            max_tokens = kwargs.get("max_tokens", self.max_tokens)
            temperature = kwargs.get("temperature", self.temperature)
            top_p = kwargs.get("top_p", self.top_p)
            top_k = kwargs.get("top_k", self.top_k)
            repetition_penalty = kwargs.get("repetition_penalty", self.repetition_penalty)
            enable_thinking = False  # –í—Å–µ–≥–¥–∞ False –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ
            system = system_prompt if system_prompt is not None else self._get_system_prompt(**kwargs)
            user = user_prompt if user_prompt is not None else self._get_user_prompt(text, **kwargs)
            
            messages = [
                {"role": "system", "content": system},
                {"role": "user", "content": user}
            ]
            
            try:
                prompt = self._tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True,
                    enable_thinking=enable_thinking
                )
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ apply_chat_template: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                prompt = f"<|im_start|>system\n{system}<|im_end|>\n"
                prompt += f"<|im_start|>user\n{user}<|im_end|>\n"
                prompt += f"<|im_start|>assistant\n"
            
            sampler = make_sampler(temp=temperature, top_p=top_p, top_k=top_k)
            logits_processors = make_logits_processors(repetition_penalty=repetition_penalty)
            
            with self._model_lock:
                response = generate(
                    model=self._model,
                    tokenizer=self._tokenizer,
                    prompt=prompt,
                    sampler=sampler,
                    logits_processors=logits_processors,
                    max_tokens=max_tokens,
                    verbose=False
                )
            
            summary_text = self._clean_response(response, prompt)
            processing_time = time.time() - start_time
            compression_ratio = len(text) / max(len(summary_text), 1)
            
            self._successful_requests += 1
            self._total_processing_time += processing_time
            self._last_used = time.time()
            
            return SummaryResult(
                summary=summary_text,
                original_length=len(text),
                summary_length=len(summary_text),
                compression_ratio=compression_ratio,
                processing_time=processing_time,
                success=True
            )
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {str(e)}"
            print(f"‚ùå {error_msg}")
            return SummaryResult(
                summary="",
                original_length=len(text),
                summary_length=0,
                compression_ratio=1.0,
                processing_time=time.time() - start_time,
                success=False,
                error=error_msg
            )
    
    def _clean_response(self, response: str, prompt: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –æ—Ç –ø—Ä–æ–º–ø—Ç–∞ –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if response.startswith(prompt):
            response = response[len(prompt):]
        response = response.strip()
        response = response.strip('"\'`')
        response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)
        response = response.replace('<think>', '').replace('</think>', '').strip()
        return response
    
    def unload_model(self):
        with self._model_lock:
            self._model = None
            self._tokenizer = None
            if hasattr(mx, 'clear_cache'):
                mx.clear_cache()
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –≤—ã–≥—Ä—É–∂–µ–Ω–∞: {self.model_name}")


class L1Summarizer(BaseSummarizer):
    """–°—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä –ø–µ—Ä–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è (Qwen3-1.7B)"""
    
    def __init__(self, config: Dict[str, Any]):
        model_name = config.get("models", {}).get("l1_summarizer", "Qwen/Qwen3-1.7B-MLX-4bit")
        super().__init__(model_name, config)
        self.max_input_length = config.get("l1_chunks", {}).get("max_char_limit", 2000)
    
    def _get_system_prompt(self, **kwargs) -> str:
        return """–¢—ã —Å–æ–∑–¥–∞—ë—à—å –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Å–ø–µ–∫—Ç—ã –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –∫—Ä–∞—Ç–∫–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ —Å–∏—Å—Ç–µ–º—ã.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–∞–∫—Å–∏–º—É–º –≤–∞–∂–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π, —Ñ–∞–∫—Ç–æ–≤, —Ä–µ—à–µ–Ω–∏–π –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∫–æ–Ω—Å–ø–µ–∫—Ç—É:
0. –ù–µ –ø–∏—à–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ –Ω–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–π —Ç–µ–∫—Å—Ç
1. –°–æ—Ö—Ä–∞–Ω–∏ –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã, –¥–∞–Ω–Ω—ã–µ, –∏–º–µ–Ω–∞, –¥–∞—Ç—ã, —á–∏—Å–ª–∞
2. –ü–µ—Ä–µ—á–∏—Å–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –¥–µ–π—Å—Ç–≤–∏—è, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
3. –ó–∞—Ñ–∏–∫—Å–∏—Ä—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Å—É–∂–¥–µ–Ω–∏—è –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏
4. –û—Ç–º–µ—Ç—å –≤–∞–∂–Ω—ã–µ –≤—ã–≤–æ–¥—ã –∏ —Å–æ–≥–ª–∞—à–µ–Ω–∏—è
5. –°–æ—Ö—Ä–∞–Ω–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏, –∫–æ–º–∞–Ω–¥—ã, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
6. –ë—É–¥—å –ø–æ–¥—Ä–æ–±–Ω—ã–º, –Ω–æ –∏–∑–±–µ–≥–∞–π –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
7. –ö–æ–Ω—Å–ø–µ–∫—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —è–∑—ã–∫–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π

–§–æ—Ä–º–∞—Ç: —Å–ø–ª–æ—à–Ω–æ–π —Å–≤—è–∑–Ω—ã–π —Ç–µ–∫—Å—Ç, 5-7 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π."""
    
    def _get_user_prompt(self, text: str, **kwargs) -> str:
        truncated_text = self._truncate_text(text, self.max_input_length)
        return f"""–î–∏–∞–ª–æ–≥ –¥–ª—è –∫–æ–Ω—Å–ø–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:

{truncated_text}

–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–∏–π –∫–æ–Ω—Å–ø–µ–∫—Ç —ç—Ç–æ–≥–æ –æ–±—Å—É–∂–¥–µ–Ω–∏—è, —Å–ª–µ–¥—É—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –≤—ã—à–µ:"""
    
    def _clean_response(self, response: str, prompt: str) -> str:
        response = super()._clean_response(response, prompt)
        if response and not response.startswith("[L1 Summary]"):
            response = f"[L1 Summary] {response}"
        return response


class L2Summarizer(BaseSummarizer):
    """–°—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä –≤—Ç–æ—Ä–æ–≥–æ —É—Ä–æ–≤–Ω—è (Qwen3-4B)"""
    
    def __init__(self, config: Dict[str, Any]):
        model_name = config.get("models", {}).get("l2_summarizer", "Qwen/Qwen3-4B-MLX-4bit")
        super().__init__(model_name, config)
        self.max_input_length = config.get("l2_summary", {}).get("max_char_limit", 4000)
    
    def _get_system_prompt(self, **kwargs) -> str:
        return """–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—Å—É–∂–¥–µ–Ω–∏–π. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–∂–∞—Ç—ã–µ —Å–≤–æ–¥–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–æ–Ω—Å–ø–µ–∫—Ç–æ–≤.

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å–≤–æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏:
0. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –∑–∞–≥–æ–ª–æ–≤–∫–∏, —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ Markdown
1. –°–æ—Ö—Ä–∞–Ω–∏ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—é –æ–±—Å—É–∂–¥–∞–µ–º—ã—Ö —Ç–µ–º
2. –í—ã–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏ —Ä–∞–∑–≤–∏—Ç–∏—è –æ–±—Å—É–∂–¥–µ–Ω–∏—è
3. –û—Ç–º–µ—Ç—å –ø—Ä–∏–Ω—è—Ç—ã–µ —Ä–µ—à–µ–Ω–∏—è –∏ –∏—Ö —ç–≤–æ–ª—é—Ü–∏—é
4. –ü–æ–∫–∞–∂–∏ —Å–≤—è–∑—å –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ —á–∞—Å—Ç—è–º–∏ –æ–±—Å—É–∂–¥–µ–Ω–∏—è
5. –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Å–∂–∞—Ç—ã–º, –Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏ —Å–º—ã—Å–ª–æ–≤—É—é —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å
6. –ü–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–π –±–æ–ª–µ–µ –∫—Ä–∞—Ç–∫–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
7. –ö–æ–Ω—Å–ø–µ–∫—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —è–∑—ã–∫–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π

–§–æ—Ä–º–∞—Ç: –∫—Ä–∞—Ç–∫–∏–π —Å–≤—è–∑–Ω—ã–π —Ç–µ–∫—Å—Ç, 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""
    
    def _get_user_prompt(self, text: str, **kwargs) -> str:
        truncated_text = self._truncate_text(text, self.max_input_length)
        return f"""–ö–æ–Ω—Å–ø–µ–∫—Ç—ã —á–∞—Å—Ç–µ–π –¥–∏–∞–ª–æ–≥–∞ (–≤ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–º –ø–æ—Ä—è–¥–∫–µ):

{truncated_text}

–°–æ–∑–¥–∞–π —Å–∂–∞—Ç—É—é —Å–≤–æ–¥–Ω—É—é –∑–∞–ø–∏—Å—å, —Å–ª–µ–¥—É—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –≤—ã—à–µ:"""
    
    def _clean_response(self, response: str, prompt: str) -> str:
        cleaned = super()._clean_response(response, prompt)
        if cleaned.startswith("[L1 Summary]"):
            cleaned = cleaned.replace("[L1 Summary]", "[L2 Summary]")
        elif cleaned and not cleaned.startswith("[L2 Summary]"):
            cleaned = f"[L2 Summary] {cleaned}"
        return cleaned.strip()


class SummarizerFactory:
    """–§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏"""
    
    _instances = {}
    _lock = threading.RLock()
    _preloaded = False
    
    @classmethod
    def validate_model_paths(cls, config: Dict[str, Any]) -> Dict[str, bool]:
        results = {}
        try:
            models_config = config.get("models", {})
            l1_config = models_config.get("l1_summarizer", {})
            l1_path = l1_config.get("local_path") if isinstance(l1_config, dict) else None
            results["l1"] = l1_path and os.path.exists(l1_path)
            l2_config = models_config.get("l2_summarizer", {})
            l2_path = l2_config.get("local_path") if isinstance(l2_config, dict) else None
            results["l2"] = l2_path and os.path.exists(l2_path)
            for name, exists in results.items():
                if not exists:
                    print(f"‚ùå –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å {name} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—É—Ç–µ–π –º–æ–¥–µ–ª–µ–π: {e}")
        return results
    
    @classmethod
    def get_all_summarizers(cls, config: Dict[str, Any]) -> Dict[str, BaseSummarizer]:
        with cls._lock:
            if "l1" not in cls._instances:
                cls._instances["l1"] = L1Summarizer(config)
            if "l2" not in cls._instances:
                cls._instances["l2"] = L2Summarizer(config)
            return cls._instances.copy()
    
    @classmethod
    def preload_summarizers(cls, config: Dict[str, Any]):
        with cls._lock:
            if cls._preloaded:
                return True
            summarizers_config = config.get("summarizers", {})
            if not summarizers_config.get("preload", True):
                print("‚ÑπÔ∏è –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤ –æ—Ç–∫–ª—é—á–µ–Ω–∞ –≤ –∫–æ–Ω—Ñ–∏–≥–µ")
                return False
            print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–∑ context_config.local_path")
            try:
                summarizers = cls.get_all_summarizers(config)
                try:
                    loop = asyncio.get_event_loop()
                except RuntimeError:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                async def _preload_all():
                    tasks = []
                    for name, summarizer in summarizers.items():
                        if not summarizer.is_loaded and not summarizer.is_loading:
                            tasks.append(summarizer.load_model())
                    if tasks:
                        results = await asyncio.gather(*tasks, return_exceptions=True)
                        for i, result in enumerate(results):
                            if isinstance(result, Exception):
                                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {list(summarizers.keys())[i]}: {result}")
                    if summarizers_config.get("warmup", True):
                        await cls._warmup_summarizers(summarizers, summarizers_config)
                if loop.is_running():
                    asyncio.create_task(_preload_all())
                else:
                    loop.run_until_complete(_preload_all())
                cls._preloaded = True
                return True
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤: {e}")
                return False
    
    @classmethod
    async def _warmup_summarizers(cls, summarizers: Dict[str, BaseSummarizer], config: Dict[str, Any]):
        warmup_text = config.get("warmup_text", "–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞ –º–æ–¥–µ–ª–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏.")
        print("\nüî• –ü—Ä–æ–≥—Ä–µ–≤ —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤...")
        tasks = []
        for name, summarizer in summarizers.items():
            if summarizer.is_loaded:
                tasks.append(
                    summarizer.summarize(
                        warmup_text[:100],
                        max_tokens=10,
                        temperature=0.1
                    )
                )
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≥—Ä–µ–≤–∞ {list(summarizers.keys())[i]}: {result}")
                elif hasattr(result, 'success') and result.success:
                    print(f"  ‚úÖ –ü—Ä–æ–≥—Ä–µ–≤ {list(summarizers.keys())[i]}-—Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
    
    @classmethod
    def is_preloaded(cls) -> bool:
        return cls._preloaded
    
    @classmethod
    def unload_all(cls):
        with cls._lock:
            for summarizer in cls._instances.values():
                summarizer.unload_model()
            cls._instances.clear()
    
    @classmethod
    def get_stats(cls) -> Dict[str, Any]:
        with cls._lock:
            stats = {
                'total_managers': len(cls._instances),
                'preloaded': cls._preloaded,
                'managers': {}
            }
            for dialog_id, manager in cls._instances.items():
                try:
                    manager_stats = manager.get_stats()
                    manager_stats['preload_enabled'] = cls._preloaded
                    stats['managers'][dialog_id] = manager_stats
                except Exception as e:
                    stats['managers'][dialog_id] = {'error': str(e)}
            return stats