# services/context/summarizers.py
"""
–ë–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤ (L1 –∏ L2).
–§–∞–±—Ä–∏–∫–∞ –≤—ã–Ω–µ—Å–µ–Ω–∞ –≤ summarizer_factory.py.
"""
import re
import time
import os
import asyncio
import threading
from typing import Dict, Any, Optional
from dataclasses import dataclass

import mlx.core as mx
from mlx_lm import generate
from mlx_lm.sample_utils import make_sampler, make_logits_processors
from container import container


@dataclass
class SummaryResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏."""
    summary: str
    original_length: int
    summary_length: int
    compression_ratio: float
    processing_time: float
    success: bool
    error: Optional[str] = None


class BaseSummarizer:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤."""

    def __init__(
        self,
        model_config: Dict[str, Any],
        config: Dict[str, Any],
        model: Optional[Any] = None,
        tokenizer: Optional[Any] = None,
        model_lock: Optional[threading.RLock] = None
    ):
        self.model_name = model_config.get("name", "unknown")
        self.local_path = model_config.get("local_path")
        self.config = config
        self._logger = None

        if model is not None and tokenizer is not None:
            self._model = model
            self._tokenizer = tokenizer
            self._model_lock = model_lock if model_lock else threading.RLock()
            self._owns_model = False
        else:
            self._model = None
            self._tokenizer = None
            self._model_lock = threading.RLock()
            self._owns_model = True

        self._is_loading = False
        self._load_error = None

        summarization_params = config.get("generation_params", {})
        model_type = "l1" if "L1" in self.__class__.__name__ else "l2"
        params = summarization_params.get(model_type, {})

        self.max_tokens = params.get("max_tokens", 200)
        self.temperature = params.get("temperature", 0.3)
        self.top_p = params.get("top_p", 0.9)
        self.top_k = params.get("top_k", 40)
        self.repetition_penalty = params.get("repetition_penalty", 1.1)

        self._total_requests = 0
        self._successful_requests = 0
        self._total_processing_time = 0.0
        self._last_used: Optional[float] = None

    @property
    def logger(self):
        if self._logger is None:
            self._logger = container.get_logger()
        return self._logger

    @property
    def is_loaded(self) -> bool:
        return self._model is not None and self._tokenizer is not None

    @property
    def stats(self) -> Dict[str, Any]:
        from datetime import datetime
        last_used_iso = None
        if self._last_used:
            try:
                last_used_iso = datetime.fromtimestamp(self._last_used).isoformat()
            except (ValueError, OSError):
                last_used_iso = str(self._last_used)
        return {
            'model_name': self.model_name,
            'is_loaded': self.is_loaded,
            'load_error': self._load_error,
            'owns_model': self._owns_model,
            'total_requests': self._total_requests,
            'successful_requests': self._successful_requests,
            'failed_requests': self._total_requests - self._successful_requests,
            'success_rate': self._successful_requests / max(self._total_requests, 1),
            'avg_processing_time': self._total_processing_time / max(self._successful_requests, 1),
            'last_used': last_used_iso,
            'generation_params': {
                'max_tokens': self.max_tokens,
                'temperature': self.temperature,
                'top_p': self.top_p,
                'top_k': self.top_k,
                'repetition_penalty': self.repetition_penalty,
                'enable_thinking': False
            }
        }

    async def load_model(self) -> bool:
        if not self._owns_model:
            return self.is_loaded
        with self._model_lock:
            if self.is_loaded:
                return True
            if self._is_loading:
                while self._is_loading:
                    await asyncio.sleep(0.1)
                return self.is_loaded

            self._is_loading = True
            self._load_error = None
            try:
                if not self.local_path:
                    self._load_error = f"–õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –Ω–µ —É–∫–∞–∑–∞–Ω –¥–ª—è –º–æ–¥–µ–ª–∏ {self.model_name}"
                    return False
                if not os.path.exists(self.local_path):
                    self._load_error = f"–õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {self.local_path}"
                    return False

                from mlx_lm import load
                start_time = time.time()
                self._model, self._tokenizer = load(self.local_path)

                if self._tokenizer.pad_token is None:
                    self._tokenizer.pad_token = self._tokenizer.eos_token
                self._tokenizer.padding_side = "left"

                self.logger.info("   ‚úÖ –ú–æ–¥–µ–ª—å %s –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ %.2f —Å–µ–∫", self.model_name, time.time() - start_time)
                return True
            except Exception as e:
                self._load_error = str(e)
                return False
            finally:
                self._is_loading = False

    async def ensure_loaded(self) -> bool:
        if not self._owns_model:
            if not self.is_loaded:
                raise RuntimeError(f"Shared model {self.model_name} is not loaded")
            return True
        if not self.is_loaded and not self.is_loading:
            return await self.load_model()
        elif self.is_loading:
            while self._is_loading:
                await asyncio.sleep(0.1)
            return self.is_loaded
        return True

    async def summarize(self, text: str, system_prompt: Optional[str] = None,
                        user_prompt: Optional[str] = None, **kwargs) -> SummaryResult:
        start_time = time.time()
        self._total_requests += 1

        try:
            if not await self.ensure_loaded():
                self.logger.error("üîç summarize: –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                return SummaryResult(
                    summary="",
                    original_length=len(text),
                    summary_length=0,
                    compression_ratio=1.0,
                    processing_time=time.time() - start_time,
                    success=False,
                    error=f"–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {self._load_error}"
                )

            max_tokens = kwargs.get("max_tokens", self.max_tokens)
            temperature = kwargs.get("temperature", self.temperature)
            top_p = kwargs.get("top_p", self.top_p)
            top_k = kwargs.get("top_k", self.top_k)
            repetition_penalty = kwargs.get("repetition_penalty", self.repetition_penalty)
            enable_thinking = False

            system = system_prompt if system_prompt is not None else self._get_system_prompt(**kwargs)
            user = user_prompt if user_prompt is not None else self._get_user_prompt(text, **kwargs)

            messages = [
                {"role": "system", "content": system},
                {"role": "user", "content": user}
            ]

            try:
                prompt = self._tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True,
                    enable_thinking=enable_thinking
                )
            except Exception as e:
                self.logger.error("üîç summarize: –æ—à–∏–±–∫–∞ apply_chat_template")
                prompt = f"<|im_start|>system\n{system}<|im_end|>\n"
                prompt += f"<|im_start|>user\n{user}<|im_end|>\n"
                prompt += f"<|im_start|>assistant\n"

            sampler = make_sampler(temp=temperature, top_p=top_p, top_k=top_k)
            logits_processors = make_logits_processors(repetition_penalty=repetition_penalty)

            with self._model_lock:
                response = generate(
                    model=self._model,
                    tokenizer=self._tokenizer,
                    prompt=prompt,
                    sampler=sampler,
                    logits_processors=logits_processors,
                    max_tokens=max_tokens,
                    verbose=False
                )

            summary_text = self._clean_response(response, prompt)
            processing_time = time.time() - start_time
            compression_ratio = len(text) / max(len(summary_text), 1)

            self._successful_requests += 1
            self._total_processing_time += processing_time
            self._last_used = time.time()

            return SummaryResult(
                summary=summary_text,
                original_length=len(text),
                summary_length=len(summary_text),
                compression_ratio=compression_ratio,
                processing_time=processing_time,
                success=True
            )
        except Exception as e:
            import traceback
            tb_str = traceback.format_exc()
            error_msg = f"–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {str(e)}\n{tb_str}"
            self.logger.error("‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –≤ summarize: %s", error_msg)
            return SummaryResult(
                summary="",
                original_length=len(text),
                summary_length=0,
                compression_ratio=1.0,
                processing_time=time.time() - start_time,
                success=False,
                error=error_msg
            )

    def _clean_response(self, response: str, prompt: str) -> str:
        if response.startswith(prompt):
            response = response[len(prompt):]
        response = response.strip()
        response = response.strip('"\'`')
        response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)
        return response

    def unload_model(self):
        with self._model_lock:
            if self._owns_model and self._model is not None:
                self._model = None
                self._tokenizer = None
                if hasattr(mx, 'clear_cache'):
                    mx.clear_cache()

    def _get_system_prompt(self, **kwargs) -> str:
        raise NotImplementedError

    def _get_user_prompt(self, text: str, **kwargs) -> str:
        raise NotImplementedError


class L1Summarizer(BaseSummarizer):
    def _get_system_prompt(self, **kwargs) -> str:
        return """–¢—ã —Å–æ–∑–¥–∞—ë—à—å –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Å–ø–µ–∫—Ç—ã –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –∫—Ä–∞—Ç–∫–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ —Å–∏—Å—Ç–µ–º—ã.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–∞–∫—Å–∏–º—É–º –≤–∞–∂–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π, —Ñ–∞–∫—Ç–æ–≤, —Ä–µ—à–µ–Ω–∏–π –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∫–æ–Ω—Å–ø–µ–∫—Ç—É:
0. –ù–µ –ø–∏—à–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ –Ω–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–π —Ç–µ–∫—Å—Ç
1. –°–æ—Ö—Ä–∞–Ω–∏ –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã, –¥–∞–Ω–Ω—ã–µ, –∏–º–µ–Ω–∞, –¥–∞—Ç—ã, —á–∏—Å–ª–∞
2. –ü–µ—Ä–µ—á–∏—Å–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –¥–µ–π—Å—Ç–≤–∏—è, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
3. –ó–∞—Ñ–∏–∫—Å–∏—Ä—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Å—É–∂–¥–µ–Ω–∏—è –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏
4. –û—Ç–º–µ—Ç—å –≤–∞–∂–Ω—ã–µ –≤—ã–≤–æ–¥—ã –∏ —Å–æ–≥–ª–∞—à–µ–Ω–∏—è
5. –°–æ—Ö—Ä–∞–Ω–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏, –∫–æ–º–∞–Ω–¥—ã, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
6. –ë—É–¥—å –ø–æ–¥—Ä–æ–±–Ω—ã–º, –Ω–æ –∏–∑–±–µ–≥–∞–π –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
7. –ö–æ–Ω—Å–ø–µ–∫—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —è–∑—ã–∫–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π

–§–æ—Ä–º–∞—Ç: —Å–ø–ª–æ—à–Ω–æ–π —Å–≤—è–∑–Ω—ã–π —Ç–µ–∫—Å—Ç, 5-7 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π."""

    def _get_user_prompt(self, text: str, **kwargs) -> str:
        max_input = self.config.get("l1_chunks", {}).get("max_char_limit", 2000)
        if len(text) > max_input:
            text = text[:max_input] + "...[—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω]"
        return f"""–î–∏–∞–ª–æ–≥ –¥–ª—è –∫–æ–Ω—Å–ø–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:

{text}

–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–∏–π –∫–æ–Ω—Å–ø–µ–∫—Ç —ç—Ç–æ–≥–æ –æ–±—Å—É–∂–¥–µ–Ω–∏—è, —Å–ª–µ–¥—É—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –≤—ã—à–µ:"""

    def _clean_response(self, response: str, prompt: str) -> str:
        cleaned = super()._clean_response(response, prompt)
        if cleaned and not cleaned.startswith("[L1 Summary]"):
            cleaned = f"[L1 Summary] {cleaned}"
        return cleaned


class L2Summarizer(BaseSummarizer):
    def _get_system_prompt(self, **kwargs) -> str:
        return """–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—Å—É–∂–¥–µ–Ω–∏–π. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–∂–∞—Ç—ã–µ —Å–≤–æ–¥–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–æ–Ω—Å–ø–µ–∫—Ç–æ–≤.

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å–≤–æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏:
0. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –∑–∞–≥–æ–ª–æ–≤–∫–∏, —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ Markdown
1. –°–æ—Ö—Ä–∞–Ω–∏ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—é –æ–±—Å—É–∂–¥–∞–µ–º—ã—Ö —Ç–µ–º
2. –í—ã–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏ —Ä–∞–∑–≤–∏—Ç–∏—è –æ–±—Å—É–∂–¥–µ–Ω–∏—è
3. –û—Ç–º–µ—Ç—å –ø—Ä–∏–Ω—è—Ç—ã–µ —Ä–µ—à–µ–Ω–∏—è –∏ –∏—Ö —ç–≤–æ–ª—é—Ü–∏—é
4. –ü–æ–∫–∞–∂–∏ —Å–≤—è–∑—å –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ —á–∞—Å—Ç—è–º–∏ –æ–±—Å—É–∂–¥–µ–Ω–∏—è
5. –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Å–∂–∞—Ç—ã–º, –Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏ —Å–º—ã—Å–ª–æ–≤—É—é —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å
6. –ü–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–π –±–æ–ª–µ–µ –∫—Ä–∞—Ç–∫–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
7. –ö–æ–Ω—Å–ø–µ–∫—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —è–∑—ã–∫–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π

–§–æ—Ä–º–∞—Ç: –∫—Ä–∞—Ç–∫–∏–π —Å–≤—è–∑–Ω—ã–π —Ç–µ–∫—Å—Ç, 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""

    def _get_user_prompt(self, text: str, **kwargs) -> str:
        max_input = self.config.get("l2_summary", {}).get("max_char_limit", 4000)
        if len(text) > max_input:
            text = text[:max_input] + "...[—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω]"
        return f"""–ö–æ–Ω—Å–ø–µ–∫—Ç—ã —á–∞—Å—Ç–µ–π –¥–∏–∞–ª–æ–≥–∞ (–≤ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–º –ø–æ—Ä—è–¥–∫–µ):

{text}

–°–æ–∑–¥–∞–π —Å–∂–∞—Ç—É—é —Å–≤–æ–¥–Ω—É—é –∑–∞–ø–∏—Å—å, —Å–ª–µ–¥—É—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –≤—ã—à–µ:"""

    def _clean_response(self, response: str, prompt: str) -> str:
        cleaned = super()._clean_response(response, prompt)
        if cleaned.startswith("[L1 Summary]"):
            cleaned = cleaned.replace("[L1 Summary]", "[L2 Summary]")
        elif cleaned and not cleaned.startswith("[L2 Summary]"):
            cleaned = f"[L2 Summary] {cleaned}"
        return cleaned.strip()