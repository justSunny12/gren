# /utils/memory_monitor.py
import psutil
import torch
import time
from typing import Dict, Any

class MemoryMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
    
    @staticmethod
    def get_memory_usage() -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏"""
        process = psutil.Process()
        memory_info = process.memory_info()
        
        result = {
            'ram_used_gb': memory_info.rss / 1024**3,
            'ram_percent': process.memory_percent(),
        }
        
        # GPU –ø–∞–º—è—Ç—å (–µ—Å–ª–∏ –µ—Å—Ç—å CUDA)
        if torch.cuda.is_available():
            result.update({
                'gpu_used_gb': torch.cuda.memory_allocated() / 1024**3,
                'gpu_cached_gb': torch.cuda.memory_reserved() / 1024**3,
                'gpu_max_used_gb': torch.cuda.max_memory_allocated() / 1024**3,
            })
        
        # MPS –ø–∞–º—è—Ç—å
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            result['device'] = 'mps'
        
        return result
    
    @staticmethod
    def print_memory_stats(prefix: str = ""):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏"""
        stats = MemoryMonitor.get_memory_usage()
        
        print(f"{prefix}üíæ RAM: {stats['ram_used_gb']:.2f} GB ({stats['ram_percent']:.1f}%)")
        
        if 'gpu_used_gb' in stats:
            print(f"{prefix}üéÆ GPU: {stats['gpu_used_gb']:.2f} GB / –∫—ç—à: {stats['gpu_cached_gb']:.2f} GB")