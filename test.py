# test_model_load.py
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def test_direct_load():
    """–ü—Ä—è–º–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
    print("üîç –¢–µ—Å—Ç –ø—Ä—è–º–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏...")
    
    model_name = "Qwen/Qwen3-4B"
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    start = time.time()
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    tokenizer_time = time.time() - start
    print(f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {tokenizer_time:.2f} —Å–µ–∫")
    
    # 2. –ú–æ–¥–µ–ª—å —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    test_cases = [
        {"low_cpu_mem_usage": False, "torch_dtype": torch.float32},
        {"low_cpu_mem_usage": True, "torch_dtype": torch.float32},
        {"low_cpu_mem_usage": True, "torch_dtype": torch.float16},
    ]
    
    for i, params in enumerate(test_cases):
        print(f"\n–¢–µ—Å—Ç {i+1}: {params}")
        try:
            start = time.time()
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                **params
            )
            
            if device == "mps":
                model = model.to("mps")
            
            load_time = time.time() - start
            print(f"  –ó–∞–≥—Ä—É–∑–∫–∞: {load_time:.2f} —Å–µ–∫")
            print(f"  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {sum(p.numel() for p in model.parameters()):,}")
            
            # –û—á–∏—Å—Ç–∫–∞
            del model
            torch.mps.empty_cache() if device == "mps" else torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"  –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    test_direct_load()